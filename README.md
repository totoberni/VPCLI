# Oxygen-Analytics: Financial Strategy CLI

## Project Overview

This command-line interface (CLI) is the user-facing application for the `Oxygen-Analytics` engine, a sophisticated tool designed to provide data-driven investment strategies for a portfolio of tech stocks (Apple, NVIDIA, Google) to a client of VP Analytics'. The tool, dubbed `VPCLI` leverages time-series analysis and machine learning to analyze the impact of financial sentiment on future stock performance.

The tool provides a hybrid analysis, offering two distinct types of strategic recommendations based on the user's selected investment horizon:
1.  **Predictive Recommendations (1M-36M):** For shorter timeframes, the CLI runs a live simulation using pre-trained XGBoost models to forecast a range of potential outcomes for various investment signals.
2.  **Historical Certainty Analysis (48M-72M):** For longer timeframes, the CLI loads pre-computed statistical analyses to report on signals that have historically demonstrated highly deterministic (i.e., high-certainty) outcomes.

---

## 1. Setup and Usage

This application is designed to be run as a self-contained Docker container, which includes all necessary dependencies and assets.

### Running with Docker (Recommended)
1.  **Pull the Image:**
    Download the latest version of the application image from Docker Hub.
    ```bash
    docker pull <your-dockerhub-username>/vpcli:latest
    ```
2.  **Run the Container:**
    Run the application in an interactive terminal session.
    ```bash
    docker run --rm -it <your-dockerhub-username>/vpcli:latest
    ```
    * The `--rm` flag removes the image from cached memory as soon as you finish using the CLI.
    * The `-it` flag is necessary, it tells Docker you want to *interact* with the CLI through the same terminal.

> `Note:` it is **not necessary** to include your dockerhub username. Commands will work fine without it as well.

---

## 2. Contributor's Guide

This guide is for developers who want to run or modify the project locally.

### 0. Dependencies

The core dependencies required to run the application are:
-   `Python 3.12.9`: Language and version for CLI development.
-   `typer`: For the command-line interface framework.
-   `rich`: For polished and readable console output (tables, colors, etc.).
-   `pandas`, `numpy`: For data manipulation and numerical operations.
-   `scikit-learn`, `xgboost-cpu`: For loading and running the pre-trained predictive models.
-   `matplotlib`: For color mapping in the display logic.

See the `pyproject.toml` file for dependency versions.

### 1. Clone the Repository
```bash
git clone <your-repository-url>
cd VPCLI
```

### 2. Extract Assets
The CLI tool operates with a set of pretrained models and predomputed data stored in `.pkl` formatt. These required `.pkl` artifacts are large and have been stored in a compressed format. Before running the application, extract the contents of the `assets.zip` file into the `VPCLI/assets/` directory at the root of the project.That's it, the CLI will now be able to load models and calculations!

### 3. Setup Virtual Environment and Install Dependencies
It is highly recommended to use a Python virtual environment.

-   **Create the environment:**
    ```bash
    # Windows
    python -m venv .venv
    # macOS / Linux
    python3 -m venv .venv
    ```
-   **Activate the environment:**
    ```bash
    # Windows (PowerShell)
    .\.venv\Scripts\Activate.ps1
    # macOS / Linux
    source .venv/bin/activate
    ```
-   **Install dependencies:**
    The project uses `pyproject.toml` to manage dependencies. Install them in editable mode (`-e`) so that changes to the source code are immediately reflected when you run the tool.
    ```bash
    pip install -e .
    ```

### 4. Run the CLI
Once the setup is complete, you can run the CLI tool directly from your terminal:
```bash
strategy-report
```

---

## 3. Architecture and Logic

The CLI is a lightweight inference engine that consumes complex data artifacts generated by a separate, offline analysis pipeline.

### Project Structure
The core application logic is organized as follows:
```
src/VPCLI/
├── __init__.py
├── core/
│   ├── __init__.py
│   ├── assets.py         # Manages loading of .pkl data artifacts.
│   ├── colors.py         # Utility for dynamic text color based on background.
│   └── constants.py      # Defines shared constants like company names and horizons.
├── display.py            # Handles all presentation logic (rendering tables).
├── engine.py             # Core business logic for simulations and analysis.
├── main.py               # Main application entrypoint and orchestrator.
└── ui.py                 # Handles all user interaction (prompts, key presses).
```

### Data Artifacts
The CLI requires three primary data artifacts, which are the output of the offline analysis described in `plan(old).md`. These must be placed in the `assets/` directory.

1.  **`trained_pipelines.pkl`**: A dictionary containing the fully trained XGBoost model suites for each company and for each short-to-medium term horizon (1M-36M). This is the "live brain" of the predictive engine.
2.  **`historical_certainty_reports.pkl`**: A dictionary containing pre-computed DataFrames that identify signals with a historically high (>95%) probability of gain for long-term horizons (48M-72M).
3.  **`engine_dfs_engineered.pkl`**: The foundational time-series dataset containing all engineered features and, crucially, the pre-calculated `Future_Return_{n}` columns for all horizons. This is used by the historical engine to find the dates on which high-certainty signals occurred.

### Core Logic

#### `engine.py` - The Strategy Engine
This module contains the primary business logic.
-   `get_predictive_report()`: Orchestrates the predictive path. It loads the relevant trained models from `trained_pipelines.pkl` and reference data from `modeling_datasets.pkl`. It then calls `simulate_investment_path()` to run a live simulation, generating a rulebook of potential investment actions by creating a matrix of scenarios and predicting their outcomes.
-   `get_monthly_signal_summary()`: Orchestrates the historical path. It identifies high-certainty signals from `historical_certainty_reports.pkl` and then finds every date those signals occurred in the `engine_dfs_engineered.pkl` data. It performs a crucial **defensive cutoff check**, ensuring that it never uses future returns that are impossible given the dataset's time boundaries. Finally, it aggregates all valid occurrences by month and returns a list of monthly summary DataFrames, filtering out any months that contain no valid data.

#### `display.py` - The Presentation Layer
This module is responsible for rendering DataFrames into human-readable console output using the `rich` library.
-   `show_predictive_report()`: Formats the results of the predictive simulation. It calculates an "Expected Value" (`ROI * Probability`) to identify and sort the most impactful signals. It then uses a color-map to generate a heatmap within the table cells, with a dynamic text-coloring utility (`core/colors.py`) to ensure readability.
-   `render_historical_page()`: Formats the aggregated monthly summaries from the historical engine. It creates a clean table for each month, displaying the signal, its value, the number of occurrences in that month, and the aggregated performance metrics (Win/Loss Ratio, Avg/Min/Max ROI).